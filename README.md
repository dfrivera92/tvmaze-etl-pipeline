# ğŸ¬ TVMaze ETL Pipeline
![Python](https://img.shields.io/badge/Python-3.8%2B-blue?logo=python&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-green)
![Workflow](https://img.shields.io/badge/Workflow-ETL_Pipeline-orange)
![Status](https://img.shields.io/badge/Status-Active-brightgreen)
## Overview
An automated ETL pipeline that extracts TV show data from TVMaze API, processes it through multiple stages, and loads it into a SQLite database.

## ğŸ—ï¸ Project structure
```plaintext
tvmaze-etl-pipeline/
â”œâ”€â”€ data/                  # Processed Parquet files
â”‚   â””â”€â”€ tvmaze_data_YYYY-MM.parquet
â”œâ”€â”€ model/                 # Image of the data model created to store the data
â”‚   â””â”€â”€ model_structure.png
â”œâ”€â”€ db/                    # SQLite database storage
â”‚   â””â”€â”€ tvmaze.db
â”œâ”€â”€ json/                  # Raw JSON data from TVMaze API
â”‚   â””â”€â”€ YYYY-MM-DD.json
â”œâ”€â”€ profiling/            # Data quality reports
â”‚   â””â”€â”€ data_profile_report.html
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ data_profiling.py
â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â”œâ”€â”€ data_export.py
â”‚   â”œâ”€â”€ data_normalization.py
â”‚   â”œâ”€â”€ db_loader.py
â”‚   â””â”€â”€ main.py
â””â”€â”€ tests/                  # Tests
    â”œâ”€â”€ test_data_cleaning
    â”œâ”€â”€ test_data_ingestion.py
    â”œâ”€â”€ test_data_processing.py
    â””â”€â”€ test_db_loader.py
```

# Key features
- ğŸ“¡ **API Integration**: Fetches TV show data from TVMaze API for any specified month
- ğŸ§¹ **Data Cleaning**: Handles missing values, duplicates, and inconsistencies
- ğŸ“Š **Data Profiling**: Generates comprehensive quality reports with `ydata-profiling`
- ğŸš€ **Efficient Storage**: Uses Parquet format for processed data
- ğŸ—„ï¸ **Relational Model**: Normalizes data into efficient SQL tables
- âš™ï¸ **Modular Design**: Each component follows single responsibility principle
- ğŸ§ª **Comprehensive Testing**: Ensures reliability with unit tests for data cleaning, ingestion, processing, and database loading using `pytest`
  
## âš™ï¸ Requirements
Install required packages
```
pip install -r requirements.txt
```

## ğŸš€ Usage
Run the ETL pipeline using:
```
python src/main.py
```

### ğŸ”„ Pipeline Workflow
The pipeline executes the following steps as shown in [`main.py`](/src/main.py):
1. Data Ingestion: [`TVMazeDataFetcher`](/src/data_ingestion.py) fetches show data from TVMaze API
2. Data Processing: [`TVMazeDataProcessor`](/src/data_processing.py) converts JSON files to DataFrame
3. Data Profiling: [`TVMazeDataProfiler`](/src/data_profiling.py) generates quality reports
4. Data Cleaning: [`TVMazeDataCleaner`](/src/data_cleaning.py) handles data quality issues
5. Data Export: [`ParquetExporter`](/src/data_export.py) saves processed data as Parquet
6. Data Normalization: [`TVMazeDataNormalizer`](/src/data_normalization.py) creates relational tables
7. Database Loading: [`SQLiteDB`](/src/db_loader.py) loads data into SQLite
8. ğŸ“Š **Data Analysis**: [`main.py`](/src/main.py#L51-L71) runs analytical queries on the dataset, including:  
   - â³ **Average Runtime of Shows**  
   - ğŸ­ **Genre Distribution**  
   - ğŸŒ **Unique Official Website Domains**  

## ğŸ“‚ Sample data (January 2024)
The repository includes sample data from January 2024:
- ğŸ“„ **Raw API Data**:
  Raw API responses stored in [`/json/2024-01-*.json`](/json/)
- ğŸ”„ **Processed Dataset**:
  Cleaned and transformed data in [`/data/tvmaze_data_2024-01.parquet`](/data/tvmaze_data_2024-01.parquet) (Parquet format)
- ğŸ“Š **Data Profile**
  HTML profile generated with Pandas Profiling available at [`/profiling/data_profile_report.html`](/profiling/data_profile_report.html)
- ğŸ“ˆ **Data Analysis**
  After processing and storing the TV show data, the pipeline performs a data analysis to extract key insights:

  ### â³ Average runtime of shows  
  The average runtime of TV shows in the dataset:  
  > **42.3 minutes**  

  ---

  ### ğŸ­ Genre distribution  
  The count of shows by genre:  

  ```plaintext
      Genre       Show Count
      ----------------------
      Undefined         261
      Drama            157
      Comedy           123
      Romance          73
      ...
  ```

  ---

  ### ğŸŒ Unique official website domains
  Some distinct official site URLs from the dataset:
  ```plaintext
    http://www.bbc.co.uk/programmes/p01djw5m
    http://www.bbc.co.uk/programmes/b006ml0g
    http://www.nbc.com/the-tonight-show
    http://www.bbc.co.uk/programmes/b006m8wd
    ...
  ```

## ğŸ›  Development
The ETL pipeline is modular, with each component handling a specific transformation:
- Each class in [`src`](/src/) follows single responsibility principle
- Data validation at each transformation step
- Comprehensive data profiling with `ydata_profiling`
- Efficient storage using Parquet format

## ğŸ›ï¸ Data Model Structure  
The **TVMaze ETL pipeline** follows a **relational model** to efficiently store TV show data. The database schema is represented in:  
ğŸ“Œ [`model_structure.png`](/model/model_structure.png)  
<div align="center"><img src="https://github.com/user-attachments/assets/09d96c15-ffdd-44b6-a7aa-043ca2adf558" width=60% height=60%></div>

## ğŸ§ª Tests
Unit tests ensure the correctness of each pipeline component. The test suite is located in the [`tests`](/tests/) directory and includes:
- âœ… **Data Cleaning**: Ensures handling of missing values, data type conversions, and renaming columns ([`test_data_cleaning.py`](/tests/test_data_cleaning.py/))
- âœ… **Data Ingestion**: Tests API fetching, invalid date handling, and data directory setup ([`test_data_ingestion.py`](/tests/test_data_ingestion.py))
- âœ… **Data Processing**: Verifies JSON to DataFrame transformation ([`test_data_processing.py`](/tests/test_data_processing.py))
- âœ… **Database Loading**: Ensures table creation and correct data insertion ([`test_db_loader.py`](/tests/test_db_loader.py))

### ğŸ” Running Tests
Run the tests using:
```
pytest tests/ -v
```

## ğŸ“œ License
This project is licensed under the terms of the [LICENSE](LICENSE) file included in the repository.
